# LLM 테스트의 중요성

[재난 현황판](https://disaster-feed.neuwappbox.com/) 백엔드에서는 재난 문자(기타로 설정된 경우)와 재난 뉴스들을 특정 유형으로 분류하는 기능이 돌아가고 있다. 세상이 좋아져서 복잡하게 분류 모델을 학습시키고 어쩌고 할 필요 없이 그냥 GPT5 Mini 모델을 API로 활용했다. 분류를 시키는 것 자체는 간단한데 원하는 성능을 유지하고 지속적으로 개선하려면 데이터셋과 평가 과정이 여전히 중요하다는 걸 느꼈다.

<!-- truncate -->

> 오늘 밤 많은 눈이 예상됩니다. 도로 결빙이 우려되오니 외출 시 미끄럼에 주의, 대중 교통 이용 및 내 집 앞 눈 치우기에도 적극적인 협조 부탁드립니다[동대문구]

이런 문자가 있다. 행안부 분류는 "기타"로 되어있는데 눈에 관한, 즉 "대설" 분류인 게 명확해 보인다. 그런데 GPT도 이걸 "기타"로 분류했다.

뭔가 심각히 잘못 분류했다는 게 느껴져서 OpenAI Platform 대시보드에서 데이터셋을 만들고 돌려보며 프롬프트를 이리저리 만지니 금세 개선할 수 있었다.

![](/attachments/blog/2026-02-01-llm-test/file-20260201203019910.png)

만약 이런 방식이 아니라 그냥 프롬프트만 보면서 "이 부분이 문제일거야", "이렇게 바꾸면 개선될거야" 하고 있었으면 점진적으로 개선하는 게 거의 불가능했을 것. 수동으로 몇가지 입력해보며 테스트하게 되면 거기로 과적합 되기 쉽기 때문. 그니까 A를 고쳤더니 갑자기 B가 안되는 상황이 나오면 안된다.

사실 말은 이렇게 했는데 진짜 제대로 하려면 데이터셋은 더 많아야 한다. 근데 뭐 그렇게 본격적으로 하긴 귀찮아서 문제 케이스가 나올 때마다 하나씩 추가하는 중. 토이 플젝에서는 이걸로도 충분한 거 같다.