# Langfuse

[Langfuse](https://langfuse.com/)는 LLM 기반 서비스 개발과 운영을 돕는 오픈소스 플랫폼이다. [OpenTelemetry](https://opentelemetry.io/) 기반으로 LLM 호출, 도구 호출 등등을 트레이싱하여 [Observability](https://langfuse.com/faq/all/llm-observability) 확보에 도움을 준다. 이 외에도 프롬프트를 버저닝하고 배포하고 평가할 수도 있다.

<!-- truncate -->

회사에서 셀프 호스팅으로 먼저 써봤는데 재난 현황판 서비스에도 적용해봤다. 기본적으로 유명한 라이브러리들에 자동으로 모니터링을 지원하긴 하는데 나는 그냥 수동으로 트레이싱 코드를 삽입했다. 그렇게 복잡하지 않음.

```typescript
// 트레이스 시작
return await startActiveObservation(
  '트레이스 이름',
  async (generation) => {
    // 트레이스 초기값 입력
    generation.update({
      input: options.messages, // LLM 입력
      model: options.model, // 사용 모델명
      modelParameters: buildModelParameters(options),
      metadata: { schemaName: options.schemaName },
    });

    try {
      // LLM 호출
      const reasoning = buildReasoningOptions(options);
      const response = await client.responses.parse(
        {
          model: options.model,
          input: toResponseInput(options.messages),
          text: { format: zodTextFormat(options.schema, options.schemaName) },
          ...(reasoning ? { reasoning } : {}),
        },
        { timeout: options.timeoutMs ?? 30000 },
      );

      const reasoningSummary = extractReasoningSummary(response.output);
      const result: LlmJsonParseResult<T> = {
        parsed: response.output_parsed ?? null,
        refusal: extractRefusal(response.output),
        reasoningSummary,
        requestId: response._request_id ?? null,
      };

      // LLM 호출 결과로 트레이스 갱신
      generation.update({
        // LLM 출력
        output: {
          parsed: result.parsed, // 파싱 결과
          refusal: result.refusal,
          reasoningSummary: result.reasoningSummary, // 추론 요약
          responseId: response.id,
        },
        // 입출력 토큰 수를 여기 넣어주면 비용 추적도 됨
        usageDetails: buildUsageDetails(response.usage),
        metadata: {
          schemaName: options.schemaName,
          responseId: response.id,
          requestId: response._request_id ?? null,
          status: response.status,
        },
      });

      return result;
    } catch (error) {
      generation.update({
        level: 'ERROR',
        statusMessage: 'OpenAI structured output request failed',
        output: { error: toErrorMessage(error) },
      });
      throw error;
    }
  },
  { asType: 'generation' },
);
```

연결하면 대시보드에서 다음과 같이 사용량을 모니터링할 수 있다.

![](/attachments/blog/2026-02-14-langfuse/file-20260214210732168.png)

그리고 "아니 이게 왜 이렇게 분류됐지?" 같은 상황에서 서버 로그를 뒤적거리는 대신 다음과 같이 직관적으로 이력을 파악할 수 있다.

![](/attachments/blog/2026-02-14-langfuse/file-20260214210900549.png)

LLM에 어떤 프롬프트가 입력되었고 추론 요약을 포함하여 어떤 출력이 나왔는지 다 보인다.

단순 취미 프로젝트라면 클라우드 서비스로 편하게 무료로 사용할 수 있다. 물론 제한이 있긴 한데 딱히 영향이 있는 정도는 아님. 클라우드가 싫거나 못 쓰는 환경이라면 셀프 호스팅하면 된다. 오픈소스라 이런 점이 좋다. 그냥 도커 컴포즈로 올려도 되는데 프로덕션급에서는 k8s 권장이긴 함.

---

2026-02-22 수정)

이제 보니 OpenAI Platform 대시보드의 Logs 메뉴에서도 확인이 가능했다. Langfuse에 비하면 정보는 조금 부족하지만 추론 내용을 포함한 입출력은 다 보여서 이걸로도 충분한 듯.